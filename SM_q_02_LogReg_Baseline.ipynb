{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Michael Bocamazo**\n",
    "\n",
    "**Question 2**: How well does logistic regression perform as a baseline with minimal feature engineering and one-hot expansion?\n",
    "\n",
    "**Date**: 2016/10/24\n",
    "\n",
    "**Methods**: Select down to only features usable within logistic regression.  Choose a few appropriate for a OHE.  Train logistic regression.  Cross-validate the ridge term.  Add in OHE feats.  Examine resultant weights, and try to identify where prediction fails.\n",
    "\n",
    "**Conclusion**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import csv\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import copy\n",
    "%matplotlib inline\n",
    "sns.set_style(\"darkgrid\", {\"grid.linewidth\": .5, \"axes.facecolor\": \".9\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ML_utils as ml_ut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous question, we have a clean data frame, and a data frame with expanded features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_c = pd.read_csv('SM_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SURVEY', 'ID', 'PURPOSE', 'FIRST', 'TICKET', 'WHO', 'LUGGAGE', 'AGE',\n",
       "       'MALE', 'INCOME', 'GA', 'ORIGIN', 'DEST', 'CAR_AV', 'TRAIN_TT',\n",
       "       'TRAIN_CO', 'TRAIN_HE', 'SM_TT', 'SM_CO', 'SM_HE', 'SM_SEATS', 'CAR_TT',\n",
       "       'CAR_CO', 'CHOICE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_e = pd.read_csv('SM_expand.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SURVEY', 'ID', 'PURPOSE', 'FIRST', 'TICKET', 'WHO', 'LUGGAGE', 'AGE',\n",
       "       'MALE', 'INCOME', 'GA', 'ORIGIN', 'DEST', 'CAR_AV', 'TRAIN_TT',\n",
       "       'TRAIN_CO', 'TRAIN_HE', 'SM_TT', 'SM_CO', 'SM_HE', 'SM_SEATS', 'CAR_TT',\n",
       "       'CAR_CO', 'CHOICE', 'min_CO', 'ratio_TRAIN_CO', 'ratio_SM_CO',\n",
       "       'ratio_CAR_CO', 'min_TT', 'ratio_TRAIN_TT', 'ratio_SM_TT',\n",
       "       'ratio_CAR_TT'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_e.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features to include\n",
    "We can make a simple list of features that would be appropriate for a logistic regression model without any transformations.  The certain ones are the numerics and binary features.  Less certain are the ordinal encoded features.  The simply encoded features require an OHE.\n",
    "\n",
    "The **pure feats** are the numeric features that we expect to get a straight correlation to choice, except for cost, which needs an adjustment based on GA to work well.  \n",
    "\n",
    "The **simple feats** are binary or ordinal encoded features.  The INCOME and AGE features both have a category for unknowns, at the endpoint.  If the latent classes behind the unknowns are equally distributed among the values, this acts as a regularizer.  In both of these features they occupy the greatest value.  They could be made to occupy the the mid value for a cleaner regularization.\n",
    "\n",
    "The **ratio features** are those developed in the first pass to compare between alternatives.  These should be quite useful for random forests, because they condense the number of nodes needed to express the comparison of the cost features.  However, they might not be useful for logistic regression.  We can experiment.\n",
    "\n",
    "Finally, there are 5 non-ordinal **encoded features** that we'll probably omit until we want much higher complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simple_feats = ['SURVEY','LUGGAGE','INCOME','AGE','GA','SM_SEATS','CAR_AV','FIRST','MALE']\n",
    "pure_feats = ['TRAIN_TT', 'TRAIN_CO', 'TRAIN_HE', 'SM_TT', 'SM_CO', 'SM_HE', 'SM_SEATS', 'CAR_TT','CAR_CO']\n",
    "encode_feats = ['PURPOSE','TICKET','WHO','ORIGIN','DEST']\n",
    "ratio_feats = ['min_CO', 'ratio_TRAIN_CO','ratio_SM_CO', 'ratio_CAR_CO', 'min_TT', 'ratio_TRAIN_TT',\n",
    "               'ratio_SM_TT','ratio_CAR_TT']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Logistic Regression\n",
    "Or, \"Multinomial Logit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df_e.drop(\"CHOICE\", axis = 1)\n",
    "y = df_e['CHOICE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The split was chosen based on session-split in q_00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split = 7002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = ml_ut.tt_split(X,y,split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we'll take the pure numeric feats and the simple ordinal encodings or binary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ML_feat = pure_feats + simple_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(multi_class='ovr').fit(Xtrain[ML_feat], ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf2 = LogisticRegression(solver = \"lbfgs\", multi_class='multinomial').fit(Xtrain[ML_feat], ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf_cv = LogisticRegressionCV(multi_class='ovr').fit(Xtrain[ML_feat], ytrain)\n",
    "clf_cv2 = LogisticRegressionCV(multi_class='multinomial').fit(Xtrain[ML_feat], ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Attribute \"C\\_\" gives the value of the regularizer that is best for each class.  Because the 'refit' parameter is default true for this model, the whole model is refit on all of the training data after finding the best hyperparameter C for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.15443469e+01,   1.00000000e-04,   1.00000000e-04])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_cv.C_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think this is due to it being the multinomial case and learning the same parameters for the whole set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.35938137,  0.35938137,  0.35938137])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_cv2.C_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "We can predict on the whole set and then evaluate on the test and train separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "py_x = clf.predict_proba(X[ML_feat])\n",
    "dy_x = clf.predict(X[ML_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.699283805597\n",
      "0.69165952585\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.log_loss(ytrain, py_x[:split]))\n",
    "print(sklearn.metrics.accuracy_score(ytrain, dy_x[:split]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.784816724333\n",
      "0.585687382298\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.log_loss(ytest, py_x[split:]))\n",
    "print(sklearn.metrics.accuracy_score(ytest, dy_x[split:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is substantially higher on the training set, which is somewhat surprising. Logistic Regression is not something that I think usually overfits.  Let's look at the other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "py_x = clf2.predict_proba(X[ML_feat])\n",
    "dy_x = clf2.predict(X[ML_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.815291533847\n",
      "0.651242502142\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.log_loss(ytrain, py_x[:split]))\n",
    "print(sklearn.metrics.accuracy_score(ytrain, dy_x[:split]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.962744442308\n",
      "0.478611783697\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.log_loss(ytest, py_x[split:]))\n",
    "print(sklearn.metrics.accuracy_score(ytest, dy_x[split:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy for the multinomial fit was worse than the one-versus-all method for the non-cross-validated case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "py_x = clf_cv.predict_proba(X[ML_feat])\n",
    "dy_x = clf_cv.predict(X[ML_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.787363181432\n",
      "0.657812053699\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.log_loss(ytrain, py_x[:split]))\n",
    "print(sklearn.metrics.accuracy_score(ytrain, dy_x[:split]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.942837909585\n",
      "0.478880817864\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.log_loss(ytest, py_x[split:]))\n",
    "print(sklearn.metrics.accuracy_score(ytest, dy_x[split:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, we do substantially better without the cross-validation.  Perhaps there is another setting that I am overlooking. I did see that the best C values were at the bounds of the search space, which could mean that the bounds need to be widened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "py_x = clf_cv2.predict_proba(X[ML_feat])\n",
    "dy_x = clf_cv2.predict(X[ML_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.806953372192\n",
      "0.651242502142\n",
      "0.953894843485\n",
      "0.477804681195\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.log_loss(ytrain, py_x[:split]))\n",
    "print(sklearn.metrics.accuracy_score(ytrain, dy_x[:split]))\n",
    "\n",
    "print(sklearn.metrics.log_loss(ytest, py_x[split:]))\n",
    "print(sklearn.metrics.accuracy_score(ytest, dy_x[split:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increase bounds of C parameter\n",
    "The best performing was the dead-simplest.  Let's try to increase the bounds on the C parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0.1,    1. ,   10. ,  100. ])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.logspace(-1,2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.802159414805\n",
      "0.659097400743\n",
      "1.00768252553\n",
      "0.474307237019\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegressionCV(Cs = np.logspace(-10,2,13)).fit(Xtrain[ML_feat], ytrain)\n",
    "\n",
    "py_x = model.predict_proba(X[ML_feat])\n",
    "dy_x = model.predict(X[ML_feat])\n",
    "print(sklearn.metrics.log_loss(ytrain, py_x[:split]))\n",
    "print(sklearn.metrics.accuracy_score(ytrain, dy_x[:split]))\n",
    "print(sklearn.metrics.log_loss(ytest, py_x[split:]))\n",
    "print(sklearn.metrics.accuracy_score(ytest, dy_x[split:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.00000000e+01,   1.00000000e-06,   1.00000000e-10])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.C_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.699283805597\n",
      "0.69165952585\n",
      "0.784816724333\n",
      "0.585687382298\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression().fit(Xtrain[ML_feat], ytrain)\n",
    "\n",
    "py_x = model.predict_proba(X[ML_feat])\n",
    "dy_x = model.predict(X[ML_feat])\n",
    "print(sklearn.metrics.log_loss(ytrain, py_x[:split]))\n",
    "print(sklearn.metrics.accuracy_score(ytrain, dy_x[:split]))\n",
    "print(sklearn.metrics.log_loss(ytest, py_x[split:]))\n",
    "print(sklearn.metrics.accuracy_score(ytest, dy_x[split:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue of C parameter\n",
    "The regularization parameter is the only real parameter to be tuned for logistic regression, besides possibly the issue of one-vs-all against multinomial.  It is possible that the first models learned with cross validation are **worse with less data**, and so they learn sub-optimal parameters.  That is the only explanation I can think of.  We can move on to the inclusion or exclusion of feature sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ML_feat = pure_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.798716559381\n",
      "0.65381319623\n",
      "0.931596683912\n",
      "0.482916330374\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression().fit(Xtrain[ML_feat], ytrain)\n",
    "\n",
    "py_x = model.predict_proba(X[ML_feat])\n",
    "dy_x = model.predict(X[ML_feat])\n",
    "print(sklearn.metrics.log_loss(ytrain, py_x[:split]))\n",
    "print(sklearn.metrics.accuracy_score(ytrain, dy_x[:split]))\n",
    "print(sklearn.metrics.log_loss(ytest, py_x[split:]))\n",
    "print(sklearn.metrics.accuracy_score(ytest, dy_x[split:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worse than before - does better with more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ML_feat = pure_feats + simple_feats + ratio_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.679408553163\n",
      "0.706369608683\n",
      "0.817640880318\n",
      "0.611783696529\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression().fit(Xtrain[ML_feat], ytrain)\n",
    "\n",
    "py_x = model.predict_proba(X[ML_feat])\n",
    "dy_x = model.predict(X[ML_feat])\n",
    "print(sklearn.metrics.log_loss(ytrain, py_x[:split]))\n",
    "print(sklearn.metrics.accuracy_score(ytrain, dy_x[:split]))\n",
    "print(sklearn.metrics.log_loss(ytest, py_x[split:]))\n",
    "print(sklearn.metrics.accuracy_score(ytest, dy_x[split:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We **do gain information** from the ratio feats, which is encouraging.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Accuracy scores from the simple random forest models\n",
    "# log loss: 0.702412421415\n",
    "# discrete accuracy: 0.684422921711"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy still has some way to go, but now that we know that the logistic regression at least gives some meaningful result, we can investigate the relative strength of features.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns to drop\n",
    "We'll make a list of the column headers that we want to drop.  Some of the data is uninformative or redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drop_cols = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['GROUP', 'SURVEY', 'SP', 'ID', 'PURPOSE', 'FIRST', 'TICKET', 'WHO',\n",
       "       'LUGGAGE', 'AGE', 'MALE', 'INCOME', 'GA', 'ORIGIN', 'DEST', 'TRAIN_AV',\n",
       "       'CAR_AV', 'SM_AV', 'TRAIN_TT', 'TRAIN_CO', 'TRAIN_HE', 'SM_TT', 'SM_CO',\n",
       "       'SM_HE', 'SM_SEATS', 'CAR_TT', 'CAR_CO', 'CHOICE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to know the meaningful data breakdown in each feature.  The 'GROUP' feature is equivalent to the 'SURVEY' feature, which encodes survey conducted in train (0) or on car trip (1).  These show the equilavence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    6759\n",
       "2    3969\n",
       "Name: GROUP, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['GROUP'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3969\n",
       "Name: SURVEY, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[df1['GROUP']==2]['SURVEY'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    6759\n",
       "Name: SURVEY, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[df1['GROUP']==3]['SURVEY'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drop_cols += ['GROUP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'TRAIN_AV' and 'SM_AV' features are not informative - always 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    10728\n",
       "Name: TRAIN_AV, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['TRAIN_AV'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    10728\n",
       "Name: SM_AV, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['SM_AV'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drop_cols += ['TRAIN_AV', 'SM_AV']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the 'CAR_AV' is meaningful, and different from where the survey was taken.  It encodes if the car is a possible output, so could be used for segmenting a modeling into two.  The only issue is sharing learning or weights between models, or tree subsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    9045\n",
       "0    1683\n",
       "Name: CAR_AV, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['CAR_AV'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column 'SP' simply means stated preference survey, is fixed at 1, and so can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    10728\n",
       "Name: SP, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['SP'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "drop_cols += ['SP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sex of the traveller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    8046\n",
       "0    2682\n",
       "Name: MALE, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['MALE'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class of travel, within a train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5679\n",
       "1    5049\n",
       "Name: FIRST, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['FIRST'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the class of the traveller is still encoded if the survey is given based on a car trip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3726\n",
       "0    3033\n",
       "Name: FIRST, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[df1['SURVEY']==1]['FIRST'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GROUP', 'TRAIN_AV', 'SM_AV', 'SP']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in drop_cols:\n",
    "    df1.drop(col, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SURVEY', 'ID', 'PURPOSE', 'FIRST', 'TICKET', 'WHO', 'LUGGAGE', 'AGE',\n",
       "       'MALE', 'INCOME', 'GA', 'ORIGIN', 'DEST', 'CAR_AV', 'TRAIN_TT',\n",
       "       'TRAIN_CO', 'TRAIN_HE', 'SM_TT', 'SM_CO', 'SM_HE', 'SM_SEATS', 'CAR_TT',\n",
       "       'CAR_CO', 'CHOICE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of Remaining Features\n",
    "Let's give a prose description of all the features that haven't yet been referenced.  **SURVEY** is the type of trip, train or car, for which the survey was given.  **ID** is the ID of the user - this is the session ID.  There are 1192 different sessions. Every row with the same ID has the same **CHOICE** value.  **PURPOSE** (9 categorical) was the purpose of the trip for which the survey was taken. **TICKET** (11 categorical) is the kind of ticket, round-trip, one-way, half-day, seasonal, etc.  **WHO** (4 categorical) is who pays: unknown, self, employer, half-half.  **LUGGAGE** (3 cat) 0 pieces, 1 piece, 3 several pieces.  **AGE** (6 cat) is an ordinal encoding of age, with greatest val unknown.  **INCOME** (5 cat) ordinal encoding of income level. **GA** - binary for owning an annual ticket. **ORIG** and **DEST** are 26-level cats encoding the region of travel.  They are probably too large to make useful, so while informative, they are unwieldy and could be dropped.  Then we have 3 types of straight numerical features - **TT** = Travel Time, in minutes, for each type, **CO** = cost, and **HE** = Headway or period between trains.  Cost for the car is computed as a fixed cost per unit distance.  Cost for the Train is computed based on the actual fare for an individual trip, or as the cost of the whole year with a GA - this makes the data quite hard to use, because there is a very clear bimodality in the distribution of costs.  The information it encodes isn't actually useful.  We would have to count on different cost thresholds being learnt for the GA/no GA cases.  Finally, **SM_SEATS** is a binary for the kinds of seats used on the Swiss Metro - airline style, or not.\n",
    "\n",
    "The numerical features we probably want to keep, and we can hypothesize that they will be quite useful in choice prediction.  The origin and destination features probably have to be dropped.  Or, only keep the most frequent 5 or so values.  Purpose and Ticket might be useful, but are still fairly large for a one-hot encoding given how many samples there are.  The ordinal categorical features could be useful, but often have codings for 'unknown' that are at an extreme and so would hurt any linear model.  They could be treated as NAs and then just be sampled from the known distribution.  However, it is probably necessary to A/B test these kinds of data-filling changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Levels of Data\n",
    "We could make several data sets: one with all of the features except for the truly unnecessary or redundant, and another, made beforehand for simplicity without the less likely features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1.to_csv('SM_clean.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After transformations, we can make another saved set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Target\n",
    "On the CHOICE value.  This probably makes sense to keep as a multioutput problem.  For every row for each ID, there is a different set of Travel Time, Costs, Headway, and other parameters that determine the desirability of each method.  \n",
    "\n",
    "It could however be transformed into a single output problem with session-level accuracy.  So each row could be converted into a set of different alternatives, each with their own travel-related parameters, but with the same customer parameters.  The target would be a binary chosen/not chosen feature.  Then, the output of the model would be a **utility**, rather than a probability, to be **normalized** to get the fractional 'shares allocation' of this user into different alternatives.  This is somewhat complicated by the fact that the different modes aren't easily comparable.  The car mode doesn't have a headway feature, and only SM has different seat configurations.  This generates about 3x the data, but a multioutput problem naturally creates models for each output.  \n",
    "\n",
    "**Different Style**\n",
    "It may be useful to train two different models: one to predict car v. not car, all car-user data goes here, then train v. SM, to which all train-only and car but train/SM choice data goes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 1192 different individuals labelling their preferences, at 10728 total combinations of alternatives presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1192"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(df1['ID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10728"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some possible appropriate machine learning feature sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ml_feat = ['SURVEY','FIRST','LUGGAGE','AGE','MALE','INCOME','GA','CAR_AV','TRAIN_TT',\n",
    "          'TRAIN_CO','TRAIN_HE','SM_TT','SM_CO','SM_HE','SM_SEATS','CAR_TT','CAR_CO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ml_feat = ['SURVEY','AGE','INCOME','GA','CAR_AV','TRAIN_TT',\n",
    "          'TRAIN_CO','TRAIN_HE','SM_TT','SM_CO','SM_HE','CAR_TT','CAR_CO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ml_feat = ['CAR_AV','TRAIN_TT','TRAIN_CO','TRAIN_HE','SM_TT','SM_CO','SM_HE','CAR_TT','CAR_CO']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminate Unknown Choice values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is little point in including a class with nine samples in the model, we could add it back in later if strictly necessary for comparison between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_k = copy.deepcopy(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_k = df_k[df_k['CHOICE']!=0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eliminates nine rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10719"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valid Minimum Cost\n",
    "We want to be able to create ratio features for the costs, which might improve learning by making direct comparisons.  For all rows, there is a train and SM cost, but there is only a car cost if the user has a car.  So to calculate a valid minimum for ratio features, we must account for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_valid_min(df):\n",
    "    x = df1.loc[:,['SM_CO','TRAIN_CO','CAR_CO']].min(axis=1)\n",
    "    for i in range(len(x)):\n",
    "        if x[i]==0:\n",
    "            x[i] = min(df1.loc[i,['SM_CO', 'TRAIN_CO']])\n",
    "    return x            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_k.loc[:,'min_CO'] = calc_valid_min(df_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratio_price_feats = ['ratio_TRAIN_CO','ratio_SM_CO','ratio_CAR_CO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_k.loc[:,'ratio_TRAIN_CO'] = df_k['TRAIN_CO']/df_k['min_CO']\n",
    "\n",
    "df_k.loc[:,'ratio_SM_CO'] = df_k['SM_CO']/df_k['min_CO']\n",
    "df_k.loc[:,'ratio_CAR_CO'] = df_k['CAR_CO']/df_k['min_CO']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valid Minimum Travel Time\n",
    "Similarly, we want to create a set of travel time ratio features so the comparison can be done directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_valid_min2(df):\n",
    "    x = df1.loc[:,['SM_TT','TRAIN_TT','CAR_TT']].min(axis=1)\n",
    "    for i in range(len(x)):\n",
    "        if x[i]==0:\n",
    "            x[i] = min(df1.loc[i,['SM_TT', 'TRAIN_TT']])\n",
    "    return x            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_k.loc[:,'min_TT'] = calc_valid_min2(df_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_k.loc[:,'ratio_TRAIN_TT'] = df_k['TRAIN_TT']/df_k['min_TT']\n",
    "df_k.loc[:,'ratio_SM_TT'] = df_k['SM_TT']/df_k['min_TT']\n",
    "df_k.loc[:,'ratio_CAR_TT'] = df_k['CAR_TT']/df_k['min_TT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ratio_feats = ratio_price_feats + ['ratio_TRAIN_TT','ratio_SM_TT','ratio_CAR_TT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ml_feat = ['SURVEY','FIRST','LUGGAGE','AGE','MALE','INCOME','GA','CAR_AV','TRAIN_TT',\n",
    "          'TRAIN_CO','TRAIN_HE','SM_TT','SM_CO','SM_HE','SM_SEATS','CAR_TT','CAR_CO'] + ratio_feats\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the result\n",
    "We'll save these likely transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_k.to_csv('SM_expand.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
